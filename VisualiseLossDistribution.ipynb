{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5e7319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "# from PreResNet import *\n",
    "# from InceptionResNetV2 import *\n",
    "from ResNet18 import ResNet18\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib import pyplot as plt\n",
    "import dataloader_AffectNet as dataloader\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b5c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_pth: str):\n",
    "    model = ResNet18(do_regr=True, do_cls=False, variance=False, pretrained=True)\n",
    "    state_dct = torch.load(model_pth, map_location=torch.device('cpu'))\n",
    "    new_state = dict()\n",
    "    for key in state_dct:\n",
    "        new_state[key.replace('module.', '')] = state_dct[key]\n",
    "    model.load_state_dict(new_state)\n",
    "    if args.multigpu:\n",
    "        # torch.cuda.set_per_process_memory_fraction(0.4, device=0)\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87da8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist(model):\n",
    "    model.eval()\n",
    "    clean_size, noisy_size = len(clean_loader.dataset), len(noisy_loader.dataset)\n",
    "    clean_losses = torch.zeros(clean_size)\n",
    "    noisy_losses = torch.zeros(noisy_size)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        num_iter = clean_size // clean_loader.batch_size + 1\n",
    "        for batch_idx, (inputs, targets, index) in enumerate(clean_loader):\n",
    "            # exp = targets[:, 2].cuda().long()\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = PSLoss(outputs, targets)\n",
    "            for b in range(inputs.size(0)):\n",
    "                clean_losses[index[b]] = loss[b]\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write('%s: Clean Data | Iter[%3d/%3d]' % (args.dataset, batch_idx + 1, num_iter))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        num_iter = noisy_size // noisy_loader.batch_size + 1\n",
    "        for batch_idx, (inputs, targets, index) in enumerate(noisy_loader):\n",
    "            # exp = targets[:, 2].cuda().long()\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = PSLoss(outputs, targets)\n",
    "            for b in range(inputs.size(0)):\n",
    "                noisy_losses[index[b]] = loss[b]\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write('%s: Noisy Data | Iter[%3d/%3d]' % (args.dataset, batch_idx + 1, num_iter))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    losses = torch.cat([clean_losses, noisy_losses])\n",
    "    losses = (losses - losses.min()) / (losses.max() - losses.min())\n",
    "\n",
    "    losses = losses.reshape(-1, 1)\n",
    "    losses = losses.cpu().numpy()\n",
    "    gmm = GaussianMixture(n_components=2, max_iter=15, reg_covar=5e-4, tol=1e-2)\n",
    "    gmm.fit(losses)\n",
    "\n",
    "    plt.hist(losses[:clean_size], bins=20, alpha=0.7, label='clean', density=False)\n",
    "    plt.hist(losses[clean_size:], bins=20, alpha=0.6, label='noisy', density=False)\n",
    "    x = np.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "    logprob = gmm.score_samples(x)\n",
    "    pdf = np.exp(logprob)\n",
    "    plt.twinx()\n",
    "    plt.plot(x, pdf, '-k', label='GMM')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa5ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data_path='/import/nobackup_mmv_ioannisp/shared/datasets/AffectNet/'\n",
    "cudnn.benchmark = True\n",
    "PSLoss = nn.L1Loss(reduction='none')\n",
    "\n",
    "clean_data = dataloader.AffectNetDataloader(\n",
    "    batch_size=batch_size,\n",
    "    num_workers=5,\n",
    "    root_dir=data_path,\n",
    "    log=None,\n",
    "    artifitial_noise='clean'\n",
    ")\n",
    "noisy_data = dataloader.AffectNetDataloader(\n",
    "    batch_size=batch_size,\n",
    "    num_workers=5,\n",
    "    root_dir=data_path,\n",
    "    log=None,\n",
    "    artifitial_noise='noisy'\n",
    ")\n",
    "clean_loader = clean_data.run(mode='eval_train')\n",
    "noisy_loader = noisy_data.run(mode='eval_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'checkpoint/20210621_1500/affectnet_lr0.5_epoch1_ensemble0_model.pth'\n",
    "net1 = create_model(model_path)\n",
    "get_hist(net1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91abc703",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'checkpoint/20210621_1500/affectnet_lr0.5_epoch4_ensemble0_model.pth'\n",
    "net1 = create_model(model_path)\n",
    "get_hist(net1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7432c653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
